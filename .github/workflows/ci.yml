name: ğŸ›ï¸ ATHintel CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run daily analysis at 3 AM UTC
    - cron: '0 3 * * *'
  workflow_dispatch:
    inputs:
      analysis_type:
        description: 'Type of analysis to run'
        required: true
        default: 'quick'
        type: choice
        options:
        - quick
        - full
        - reports
        - validation

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.8, 3.9, '3.10']
    
    steps:
    - name: ğŸš€ Checkout Repository
      uses: actions/checkout@v4
    
    - name: ğŸ Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'
    
    - name: ğŸ“¦ Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r config/requirements.txt
        pip install pytest pytest-cov pytest-asyncio black flake8 mypy
    
    - name: ğŸ” Code Quality Checks
      run: |
        echo "ğŸ” Running code quality checks..."
        black --check --diff .
        flake8 . --max-line-length=88 --exclude=.git,__pycache__,venv
        mypy core/ --ignore-missing-imports
    
    - name: ğŸ§ª Run Tests
      run: |
        echo "ğŸ§ª Running test suite..."
        python -m pytest tests/ -v --cov=core --cov-report=xml --cov-report=html
    
    - name: ğŸ“Š Upload Coverage Reports
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella

  analysis:
    runs-on: ubuntu-latest
    needs: test
    if: github.event_name == 'schedule' || github.event.inputs.analysis_type
    
    steps:
    - name: ğŸš€ Checkout Repository
      uses: actions/checkout@v4
    
    - name: ğŸ Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        cache: 'pip'
    
    - name: ğŸ“¦ Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r config/requirements.txt
    
    - name: âš¡ Run Quick Analysis
      if: github.event.inputs.analysis_type == 'quick' || github.event.inputs.analysis_type == ''
      run: |
        echo "âš¡ Running quick analysis with sample data..."
        python main.py --mode quick --areas Kolonaki Koukaki Plaka
    
    - name: ğŸ“‹ Generate Reports
      if: github.event.inputs.analysis_type == 'reports'
      run: |
        echo "ğŸ“‹ Generating investment reports..."
        python main.py --mode reports --data data/sample/properties.json
    
    - name: âœ… Validate Platform
      if: github.event.inputs.analysis_type == 'validation'
      run: |
        echo "âœ… Running platform validation..."
        python -c "
        import sys
        from pathlib import Path
        
        # Validate core modules
        try:
            from core.collectors.athens_comprehensive_collector import AthensComprehensiveCollector
            from core.intelligence.investment_engine import InvestmentIntelligenceEngine
            from reports.investment.investment_report_generator import InvestmentReportGenerator
            print('âœ… All core modules imported successfully')
        except ImportError as e:
            print(f'âŒ Import error: {e}')
            sys.exit(1)
        
        # Validate configuration
        config_file = Path('config/platform_config.json')
        if config_file.exists():
            print('âœ… Configuration file exists')
        else:
            print('âŒ Configuration file missing')
            sys.exit(1)
        
        # Validate sample data
        sample_data = Path('data/sample/properties.json')
        if sample_data.exists():
            print('âœ… Sample data exists')
        else:
            print('âŒ Sample data missing')
            sys.exit(1)
            
        print('ğŸ¯ Platform validation completed successfully')
        "
    
    - name: ğŸ“Š Archive Analysis Results
      uses: actions/upload-artifact@v3
      with:
        name: analysis-results-${{ github.run_number }}
        path: |
          reports/investment/*.md
          reports/investment/*.json
          data/processed/*.json
        retention-days: 30
    
    - name: ğŸ“ˆ Update Analysis Badge
      if: success()
      run: |
        mkdir -p .github/badges
        echo "![Analysis](https://img.shields.io/badge/Analysis-Passing-brightgreen?style=flat-square&logo=chart-line)" > .github/badges/analysis.md
        echo "![Last Run](https://img.shields.io/badge/Last_Run-$(date +'%Y-%m-%d')-blue?style=flat-square&logo=calendar)" >> .github/badges/analysis.md

  security:
    runs-on: ubuntu-latest
    
    steps:
    - name: ğŸš€ Checkout Repository
      uses: actions/checkout@v4
    
    - name: ğŸ”’ Run Security Scan
      uses: github/super-linter@v4
      env:
        DEFAULT_BRANCH: main
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        VALIDATE_PYTHON_BLACK: false
        VALIDATE_PYTHON_FLAKE8: false
        VALIDATE_PYTHON_MYPY: false
    
    - name: ğŸ›¡ï¸ Security Audit
      run: |
        echo "ğŸ›¡ï¸ Running security audit..."
        pip install safety bandit
        safety check -r config/requirements.txt
        bandit -r core/ -f json -o security-report.json || true
    
    - name: ğŸ“‹ Upload Security Report
      uses: actions/upload-artifact@v3
      with:
        name: security-report-${{ github.run_number }}
        path: security-report.json
        retention-days: 30

  build-docs:
    runs-on: ubuntu-latest
    
    steps:
    - name: ğŸš€ Checkout Repository
      uses: actions/checkout@v4
    
    - name: ğŸ Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: ğŸ“š Build Documentation
      run: |
        echo "ğŸ“š Building documentation..."
        pip install mkdocs mkdocs-material
        # mkdocs build (when we add mkdocs.yml)
        echo "Documentation build completed"
    
    - name: ğŸš€ Deploy to GitHub Pages
      if: github.ref == 'refs/heads/main'
      uses: peaceiris/actions-gh-pages@v3
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        publish_dir: ./site

  performance:
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'
    
    steps:
    - name: ğŸš€ Checkout Repository
      uses: actions/checkout@v4
    
    - name: ğŸ Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        cache: 'pip'
    
    - name: ğŸ“¦ Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r config/requirements.txt
        pip install memory-profiler psutil
    
    - name: ğŸ“Š Performance Benchmarks
      run: |
        echo "ğŸ“Š Running performance benchmarks..."
        python -c "
        import time
        import psutil
        import json
        from datetime import datetime
        
        # Simulate quick analysis performance test
        start_time = time.time()
        start_memory = psutil.virtual_memory().used / 1024 / 1024  # MB
        
        # Import core modules (simulates loading)
        from core.intelligence.investment_engine import InvestmentIntelligenceEngine
        
        # Create engine instance
        engine = InvestmentIntelligenceEngine()
        
        end_time = time.time()
        end_memory = psutil.virtual_memory().used / 1024 / 1024  # MB
        
        performance_metrics = {
            'timestamp': datetime.now().isoformat(),
            'load_time_seconds': end_time - start_time,
            'memory_usage_mb': end_memory - start_memory,
            'python_version': '3.9',
            'platform': 'ubuntu-latest'
        }
        
        with open('performance-metrics.json', 'w') as f:
            json.dump(performance_metrics, f, indent=2)
        
        print(f'âš¡ Load time: {performance_metrics[\"load_time_seconds\"]:.2f} seconds')
        print(f'ğŸ’¾ Memory usage: {performance_metrics[\"memory_usage_mb\"]:.1f} MB')
        "
    
    - name: ğŸ“Š Upload Performance Metrics
      uses: actions/upload-artifact@v3
      with:
        name: performance-metrics-${{ github.run_number }}
        path: performance-metrics.json
        retention-days: 90

  notify:
    runs-on: ubuntu-latest
    needs: [test, analysis, security]
    if: always()
    
    steps:
    - name: ğŸ“¢ Workflow Summary
      run: |
        echo "ğŸ›ï¸ ATHintel CI/CD Pipeline Summary"
        echo "=================================="
        echo "ğŸ“Š Test Status: ${{ needs.test.result }}"
        echo "ğŸ§  Analysis Status: ${{ needs.analysis.result }}"
        echo "ğŸ”’ Security Status: ${{ needs.security.result }}"
        echo "ğŸ¯ Workflow: ${{ github.workflow }}"
        echo "ğŸŒ¿ Branch: ${{ github.ref_name }}"
        echo "ğŸ‘¤ Actor: ${{ github.actor }}"
        echo "ğŸ“… Timestamp: $(date)"
        
        if [ "${{ needs.test.result }}" == "success" ] && [ "${{ needs.security.result }}" == "success" ]; then
          echo "âœ… Pipeline completed successfully"
        else
          echo "âŒ Pipeline completed with issues"
        fi